(window.webpackJsonp=window.webpackJsonp||[]).push([[10],{190:function(t,s,a){"use strict";a.r(s);var n=a(0),r=Object(n.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("div",{staticClass:"content"},[a("h1",{attrs:{id:"项目结构指南"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#项目结构指南","aria-hidden":"true"}},[t._v("#")]),t._v(" 项目结构指南")]),t._v(" "),a("p"),a("div",{staticClass:"table-of-contents"},[a("ul",[a("li",[a("a",{attrs:{href:"#小实验"}},[t._v("小实验")])]),a("li",[a("a",{attrs:{href:"#将变化拆分到各文件"}},[t._v("将变化拆分到各文件")]),a("ul",[a("li",[a("a",{attrs:{href:"#扩展数据集"}},[t._v("扩展数据集")])]),a("li",[a("a",{attrs:{href:"#扩展模型结构"}},[t._v("扩展模型结构")])]),a("li",[a("a",{attrs:{href:"#扩展-trick"}},[t._v("扩展 trick")])])])]),a("li",[a("a",{attrs:{href:"#什么时候需要多个trainer？"}},[t._v("什么时候需要多个Trainer？")])]),a("li",[a("a",{attrs:{href:"#在全局使用-params"}},[t._v("在全局使用 params")]),a("ul",[a("li",[a("a",{attrs:{href:"#是否有必要定义多个-params"}},[t._v("是否有必要定义多个 Params")])])])]),a("li",[a("a",{attrs:{href:"#扩展thexp功能"}},[t._v("扩展thexp功能")])])])]),a("p"),t._v(" "),a("p",[t._v("本章提供对小型（单个模型的训练）、中型项目（多个模型的训练）的代码结构的一些思考和示例。")]),t._v(" "),a("h2",{attrs:{id:"小实验"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#小实验","aria-hidden":"true"}},[t._v("#")]),t._v(" 小实验")]),t._v(" "),a("p",[t._v("一个小实验，可能只需要一个文件，数据集、模型、训练算法写在一起，很多这样的实验都会有五六百行，而且是通过传递训练方法的方式来进行训练，这种方式很不利于定位核心代码，每定位一次变量的定义可能都要定位好几次，而这在方法名相同的情况下就更难受了。")]),t._v(" "),a("p",[t._v("下面展示一个利用 "),a("code",[t._v("thexp")]),t._v(" 完成的，代码结构良好的MNIST训练过程，这是接下来其他示例的基础：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{attrs:{class:"token keyword"}},[t._v("from")]),t._v(" torch"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn "),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" functional "),a("span",{attrs:{class:"token keyword"}},[t._v("as")]),t._v(" F\n"),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn "),a("span",{attrs:{class:"token keyword"}},[t._v("as")]),t._v(" nn\n"),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\n"),a("span",{attrs:{class:"token keyword"}},[t._v("from")]),t._v(" thexp "),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("*")]),t._v("\n\n"),a("span",{attrs:{class:"token comment"}},[t._v("# glob.add_value('datasets', '/home/share/yanghaozhe/pytorchdataset', glob.LEVEL.repository)")]),t._v("\n\nparams "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" Params"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nparams"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("device "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v("'cuda:1'")]),t._v("\nparams"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("epoch "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("5")]),t._v("\nparams"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("batch_size "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("128")]),t._v("\nparams"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("topk "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token number"}},[t._v("1")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("4")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nparams"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_args"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{attrs:{class:"token comment"}},[t._v("# params.root = glob['datasets']")]),t._v("\nparams"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("root "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v("'/home/share/yanghaozhe/pytorchdataset'")]),t._v("\nparams"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloader "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token builtin"}},[t._v("dict")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("shuffle"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token boolean"}},[t._v("True")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" batch_size"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token number"}},[t._v("32")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" drop_last"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token boolean"}},[t._v("True")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nparams"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optim "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token builtin"}},[t._v("dict")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("lr"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token number"}},[t._v("0.01")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weight_decay"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token number"}},[t._v("0.09")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" momentum"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token number"}},[t._v("0.9")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("MNIST")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mode"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token string"}},[t._v('"train"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" download"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token boolean"}},[t._v("False")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{attrs:{class:"token comment"}},[t._v("# 由于不同数据集区分训练或测试集的方式不同，因此建议数据集以统一的接口定义")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("from")]),t._v(" torchvision"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("datasets"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mnist "),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" MNIST\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("from")]),t._v(" torchvision "),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" transforms\n    weak "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" transforms"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ToTensor"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("if")]),t._v(" mode "),a("span",{attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"train"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" MNIST"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("params"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("root"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token boolean"}},[t._v("True")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" download"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("download"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" transform"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("weak"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("else")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" MNIST"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("params"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("root"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token boolean"}},[t._v("False")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" transform"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("weak"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" download"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("download"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),a("span",{attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{attrs:{class:"token class-name"}},[t._v("SimpleNet")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nn"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Module"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{attrs:{class:"token triple-quoted-string string"}},[t._v('"""模型定义部分"""')]),t._v("\n\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("__init__")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{attrs:{class:"token builtin"}},[t._v("super")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("SimpleNet"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("convnet "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Sequential"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nn"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Conv2d"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token number"}},[t._v("1")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("32")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("5")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" nn"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("PReLU"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                     nn"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("MaxPool2d"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token number"}},[t._v("2")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" stride"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token number"}},[t._v("2")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                     nn"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Conv2d"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token number"}},[t._v("32")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("64")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("5")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" nn"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("PReLU"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                     nn"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("MaxPool2d"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token number"}},[t._v("2")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" stride"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token number"}},[t._v("2")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fc "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Sequential"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nn"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token number"}},[t._v("64")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("4")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("4")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("256")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                nn"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Tanh"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                nn"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token number"}},[t._v("256")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("128")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                nn"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Tanh"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                nn"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token number"}},[t._v("128")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("10")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                                "),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("forward")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        output "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("convnet"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        output "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" output"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("view"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token number"}},[t._v("0")]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("-")]),a("span",{attrs:{class:"token number"}},[t._v("1")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        output "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fc"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" output\n\n\n"),a("span",{attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{attrs:{class:"token class-name"}},[t._v("MNISTTrainer")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Trainer"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    __exp_name__ "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"MNISTDemo"')]),t._v("\n\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("callbacks")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" params"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Params"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        callbacks"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("LoggerCallback"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hook"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        callbacks"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("AutoRecord"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hook"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        callbacks"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("EvalCallback"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_per_epoch"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token number"}},[t._v("5")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hook"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("datasets")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" params"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Params"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("from")]),t._v(" torch"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("utils"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloader "),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" DataLoader\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("from")]),t._v(" thexp"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("torch"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("collate "),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" AutoCollate\n\n        "),a("span",{attrs:{class:"token comment"}},[t._v("# self.device = torch.device(param.device)")]),t._v("\n        train_loader "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataLoader"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("MNIST"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mode"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token string"}},[t._v("'train'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("**")]),t._v("params"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloader"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" collate_fn"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("AutoCollate"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("device"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        test_loader "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataLoader"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("MNIST"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mode"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token string"}},[t._v("'test'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("**")]),t._v("params"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloader"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" collate_fn"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("AutoCollate"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("device"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("regist_databundler"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n            train"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("train_loader"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            test"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("test_loader"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("models")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" params"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Params"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("from")]),t._v(" torch"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optim "),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SGD\n        self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" SimpleNet"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("device"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optim "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" SGD"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parameters"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("**")]),t._v("params"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optim"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("train_batch")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" eidx"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" idx"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" global_step"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" batch_data"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" params"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" device"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        optim "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optim "),a("span",{attrs:{class:"token comment"}},[t._v("# 你可以直接调用 self.optim，也可以用这种方式，看你喜欢哪一种")]),t._v("\n        meter "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" Meter"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        xs"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ys "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" batch_data\n\n        "),a("span",{attrs:{class:"token comment"}},[t._v("# 训练逻辑")]),t._v("\n        logits "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xs"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        meter"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loss "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" F"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cross_entropy"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("logits"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ys"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train_precise"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("logits"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ys"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" meter"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{attrs:{class:"token comment"}},[t._v("# 反向传播")]),t._v("\n        meter"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loss"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("backward"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        optim"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("step"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        optim"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zero_grad"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" meter\n\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("train_precise")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" logits"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" labels"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" meter"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{attrs:{class:"token triple-quoted-string string"}},[t._v('"""train batch accuracy"""')]),t._v("\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("with")]),t._v(" torch"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("no_grad"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            _"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" maxid "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token builtin"}},[t._v("max")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("logits"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dim"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token operator"}},[t._v("-")]),a("span",{attrs:{class:"token number"}},[t._v("1")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            total "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" labels"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token number"}},[t._v("0")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            top1 "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("labels "),a("span",{attrs:{class:"token operator"}},[t._v("==")]),t._v(" maxid"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token builtin"}},[t._v("sum")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("item"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            meter"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("acc "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" top1 "),a("span",{attrs:{class:"token operator"}},[t._v("/")]),t._v(" total\n            meter"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("percent"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("meter"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("acc_"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" meter"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("acc\n\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("test_eval_logic")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dataloader"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" param"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Params"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("from")]),t._v(" thexp"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("calculate "),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" accuracy "),a("span",{attrs:{class:"token keyword"}},[t._v("as")]),t._v(" acc\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("with")]),t._v(" torch"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("no_grad"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            count_dict "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" Meter"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" xs"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" labels "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" dataloader"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                xs"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" labels "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" xs"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("device"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" labels"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("device"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                preds "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xs"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                total"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" topk_res "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" acc"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("classify"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preds"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" labels"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" topk"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("param"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("topk"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                count_dict"),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token string"}},[t._v('"total"')]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("+=")]),t._v(" total\n                "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" topi_res "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{attrs:{class:"token builtin"}},[t._v("zip")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("param"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("topk"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" topk_res"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    count_dict"),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("+=")]),t._v(" topi_res\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" count_dict\n\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("predict")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" xs"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xs"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),a("span",{attrs:{class:"token comment"}},[t._v("# params")]),t._v("\n"),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" p "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" params"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("grid_search"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token string"}},[t._v('"optim.lr"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token number"}},[t._v("0.001")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("0.005")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("0.0005")]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" pp "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" p"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("grid_search"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token string"}},[t._v('"epoch"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token number"}},[t._v("10")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("15")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("20")]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        trainer "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" MNISTTrainer"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pp"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        trainer"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br"),a("span",{staticClass:"line-number"},[t._v("24")]),a("br"),a("span",{staticClass:"line-number"},[t._v("25")]),a("br"),a("span",{staticClass:"line-number"},[t._v("26")]),a("br"),a("span",{staticClass:"line-number"},[t._v("27")]),a("br"),a("span",{staticClass:"line-number"},[t._v("28")]),a("br"),a("span",{staticClass:"line-number"},[t._v("29")]),a("br"),a("span",{staticClass:"line-number"},[t._v("30")]),a("br"),a("span",{staticClass:"line-number"},[t._v("31")]),a("br"),a("span",{staticClass:"line-number"},[t._v("32")]),a("br"),a("span",{staticClass:"line-number"},[t._v("33")]),a("br"),a("span",{staticClass:"line-number"},[t._v("34")]),a("br"),a("span",{staticClass:"line-number"},[t._v("35")]),a("br"),a("span",{staticClass:"line-number"},[t._v("36")]),a("br"),a("span",{staticClass:"line-number"},[t._v("37")]),a("br"),a("span",{staticClass:"line-number"},[t._v("38")]),a("br"),a("span",{staticClass:"line-number"},[t._v("39")]),a("br"),a("span",{staticClass:"line-number"},[t._v("40")]),a("br"),a("span",{staticClass:"line-number"},[t._v("41")]),a("br"),a("span",{staticClass:"line-number"},[t._v("42")]),a("br"),a("span",{staticClass:"line-number"},[t._v("43")]),a("br"),a("span",{staticClass:"line-number"},[t._v("44")]),a("br"),a("span",{staticClass:"line-number"},[t._v("45")]),a("br"),a("span",{staticClass:"line-number"},[t._v("46")]),a("br"),a("span",{staticClass:"line-number"},[t._v("47")]),a("br"),a("span",{staticClass:"line-number"},[t._v("48")]),a("br"),a("span",{staticClass:"line-number"},[t._v("49")]),a("br"),a("span",{staticClass:"line-number"},[t._v("50")]),a("br"),a("span",{staticClass:"line-number"},[t._v("51")]),a("br"),a("span",{staticClass:"line-number"},[t._v("52")]),a("br"),a("span",{staticClass:"line-number"},[t._v("53")]),a("br"),a("span",{staticClass:"line-number"},[t._v("54")]),a("br"),a("span",{staticClass:"line-number"},[t._v("55")]),a("br"),a("span",{staticClass:"line-number"},[t._v("56")]),a("br"),a("span",{staticClass:"line-number"},[t._v("57")]),a("br"),a("span",{staticClass:"line-number"},[t._v("58")]),a("br"),a("span",{staticClass:"line-number"},[t._v("59")]),a("br"),a("span",{staticClass:"line-number"},[t._v("60")]),a("br"),a("span",{staticClass:"line-number"},[t._v("61")]),a("br"),a("span",{staticClass:"line-number"},[t._v("62")]),a("br"),a("span",{staticClass:"line-number"},[t._v("63")]),a("br"),a("span",{staticClass:"line-number"},[t._v("64")]),a("br"),a("span",{staticClass:"line-number"},[t._v("65")]),a("br"),a("span",{staticClass:"line-number"},[t._v("66")]),a("br"),a("span",{staticClass:"line-number"},[t._v("67")]),a("br"),a("span",{staticClass:"line-number"},[t._v("68")]),a("br"),a("span",{staticClass:"line-number"},[t._v("69")]),a("br"),a("span",{staticClass:"line-number"},[t._v("70")]),a("br"),a("span",{staticClass:"line-number"},[t._v("71")]),a("br"),a("span",{staticClass:"line-number"},[t._v("72")]),a("br"),a("span",{staticClass:"line-number"},[t._v("73")]),a("br"),a("span",{staticClass:"line-number"},[t._v("74")]),a("br"),a("span",{staticClass:"line-number"},[t._v("75")]),a("br"),a("span",{staticClass:"line-number"},[t._v("76")]),a("br"),a("span",{staticClass:"line-number"},[t._v("77")]),a("br"),a("span",{staticClass:"line-number"},[t._v("78")]),a("br"),a("span",{staticClass:"line-number"},[t._v("79")]),a("br"),a("span",{staticClass:"line-number"},[t._v("80")]),a("br"),a("span",{staticClass:"line-number"},[t._v("81")]),a("br"),a("span",{staticClass:"line-number"},[t._v("82")]),a("br"),a("span",{staticClass:"line-number"},[t._v("83")]),a("br"),a("span",{staticClass:"line-number"},[t._v("84")]),a("br"),a("span",{staticClass:"line-number"},[t._v("85")]),a("br"),a("span",{staticClass:"line-number"},[t._v("86")]),a("br"),a("span",{staticClass:"line-number"},[t._v("87")]),a("br"),a("span",{staticClass:"line-number"},[t._v("88")]),a("br"),a("span",{staticClass:"line-number"},[t._v("89")]),a("br"),a("span",{staticClass:"line-number"},[t._v("90")]),a("br"),a("span",{staticClass:"line-number"},[t._v("91")]),a("br"),a("span",{staticClass:"line-number"},[t._v("92")]),a("br"),a("span",{staticClass:"line-number"},[t._v("93")]),a("br"),a("span",{staticClass:"line-number"},[t._v("94")]),a("br"),a("span",{staticClass:"line-number"},[t._v("95")]),a("br"),a("span",{staticClass:"line-number"},[t._v("96")]),a("br"),a("span",{staticClass:"line-number"},[t._v("97")]),a("br"),a("span",{staticClass:"line-number"},[t._v("98")]),a("br"),a("span",{staticClass:"line-number"},[t._v("99")]),a("br"),a("span",{staticClass:"line-number"},[t._v("100")]),a("br"),a("span",{staticClass:"line-number"},[t._v("101")]),a("br"),a("span",{staticClass:"line-number"},[t._v("102")]),a("br"),a("span",{staticClass:"line-number"},[t._v("103")]),a("br"),a("span",{staticClass:"line-number"},[t._v("104")]),a("br"),a("span",{staticClass:"line-number"},[t._v("105")]),a("br"),a("span",{staticClass:"line-number"},[t._v("106")]),a("br"),a("span",{staticClass:"line-number"},[t._v("107")]),a("br"),a("span",{staticClass:"line-number"},[t._v("108")]),a("br"),a("span",{staticClass:"line-number"},[t._v("109")]),a("br"),a("span",{staticClass:"line-number"},[t._v("110")]),a("br"),a("span",{staticClass:"line-number"},[t._v("111")]),a("br"),a("span",{staticClass:"line-number"},[t._v("112")]),a("br"),a("span",{staticClass:"line-number"},[t._v("113")]),a("br"),a("span",{staticClass:"line-number"},[t._v("114")]),a("br"),a("span",{staticClass:"line-number"},[t._v("115")]),a("br"),a("span",{staticClass:"line-number"},[t._v("116")]),a("br"),a("span",{staticClass:"line-number"},[t._v("117")]),a("br"),a("span",{staticClass:"line-number"},[t._v("118")]),a("br"),a("span",{staticClass:"line-number"},[t._v("119")]),a("br"),a("span",{staticClass:"line-number"},[t._v("120")]),a("br"),a("span",{staticClass:"line-number"},[t._v("121")]),a("br"),a("span",{staticClass:"line-number"},[t._v("122")]),a("br"),a("span",{staticClass:"line-number"},[t._v("123")]),a("br"),a("span",{staticClass:"line-number"},[t._v("124")]),a("br"),a("span",{staticClass:"line-number"},[t._v("125")]),a("br"),a("span",{staticClass:"line-number"},[t._v("126")]),a("br"),a("span",{staticClass:"line-number"},[t._v("127")]),a("br"),a("span",{staticClass:"line-number"},[t._v("128")]),a("br")])]),a("h2",{attrs:{id:"将变化拆分到各文件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#将变化拆分到各文件","aria-hidden":"true"}},[t._v("#")]),t._v(" 将变化拆分到各文件")]),t._v(" "),a("p",[t._v("里面的很多地方都是会变的，比如，你可能会需要尝试多个数据集，可能会使用多个模型，可能会需要更换使用多个损失函数...")]),t._v(" "),a("p",[t._v("对多变的部份，建议将其额外拆分到一个单文件中。")]),t._v(" "),a("p",[t._v("以上述的文件为例，假设原本的目录结构为：")]),t._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("└─ main.py\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("h3",{attrs:{id:"扩展数据集"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#扩展数据集","aria-hidden":"true"}},[t._v("#")]),t._v(" 扩展数据集")]),t._v(" "),a("p",[t._v("如果你除了要训练MNIST，也要训练FashionMNIST，那么就建议你将两个数据集的读取部份单独放在一个文件里：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{attrs:{class:"token keyword"}},[t._v("from")]),t._v(" thexp "),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" glob\n\nroot "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" glob"),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token string"}},[t._v("'datasets'")]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n"),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("MNIST")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mode"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token string"}},[t._v('"train"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" download"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token boolean"}},[t._v("False")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{attrs:{class:"token comment"}},[t._v("# 由于不同数据集区分训练或测试集的方式不同，因此建议数据集以统一的接口定义")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("from")]),t._v(" torchvision"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("datasets"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mnist "),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" MNIST\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("from")]),t._v(" torchvision "),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" transforms\n    weak "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" transforms"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ToTensor"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("if")]),t._v(" mode "),a("span",{attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"train"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" MNIST"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("root"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token boolean"}},[t._v("True")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" download"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("download"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" transform"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("weak"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("else")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" MNIST"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("root"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token boolean"}},[t._v("False")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" transform"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("weak"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" download"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("download"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("FMNIST")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mode"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token string"}},[t._v('"train"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" download"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token boolean"}},[t._v("False")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{attrs:{class:"token comment"}},[t._v("# 由于不同数据集区分训练或测试集的方式不同，因此建议数据集以统一的接口定义")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("from")]),t._v(" torchvision"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("datasets"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mnist "),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" FashionMNIST\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("from")]),t._v(" torchvision "),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" transforms\n    weak "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" transforms"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ToTensor"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("if")]),t._v(" mode "),a("span",{attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"train"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" FashionMNIST"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("root"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token boolean"}},[t._v("True")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" download"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("download"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" transform"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("weak"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("else")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" FashionMNIST"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("root"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token boolean"}},[t._v("False")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" transform"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("weak"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" download"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("download"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br"),a("span",{staticClass:"line-number"},[t._v("24")]),a("br")])]),a("p",[t._v("你的参数定义可能会多处这样的一条：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("params"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("choice"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token string"}},[t._v("'dataset'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v("'mnist'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v("'fmnist'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("p",[t._v("在"),a("code",[t._v("Trainer")]),t._v("的数据集读取中可能会存在分支逻辑：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("div",{staticClass:"highlight-lines"},[a("br"),a("br"),a("br"),a("br"),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("br"),a("br"),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("br"),a("br"),a("br"),a("br"),a("br"),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("br"),a("br"),a("br"),a("br"),a("br"),a("br")]),a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("    "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("datasets")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" params"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Params"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("from")]),t._v(" torch"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("utils"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloader "),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" DataLoader\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("from")]),t._v(" thexp"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("torch"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("collate "),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" AutoCollate\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("from")]),t._v(" datasets "),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" MNIST"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" FMNIST\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("if")]),t._v(" params"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataset "),a("span",{attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v("'mnist'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            train_loader "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataLoader"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("MNIST"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mode"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token string"}},[t._v("'train'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("**")]),t._v("params"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloader"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" collate_fn"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("AutoCollate"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("device"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            test_loader "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataLoader"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("MNIST"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mode"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token string"}},[t._v("'test'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("**")]),t._v("params"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloader"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" collate_fn"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("AutoCollate"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("device"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" params"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataset "),a("span",{attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v("'fmnist'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            train_loader "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataLoader"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("FMNIST"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mode"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token string"}},[t._v("'train'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("**")]),t._v("params"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloader"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                      collate_fn"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("AutoCollate"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("device"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            test_loader "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataLoader"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("FMNIST"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mode"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token string"}},[t._v("'test'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("**")]),t._v("params"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloader"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                     collate_fn"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("AutoCollate"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("device"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("else")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{attrs:{class:"token boolean"}},[t._v("False")]),t._v("\n\n        self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("regist_databundler"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n            train"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("train_loader"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            test"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("test_loader"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br")])]),a("div",{staticClass:"tip custom-block"},[a("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),a("p",[t._v("你会看到对 dataset 的判断均显式的进行了值的判断，并在最后的 else 逻辑中加入了 "),a("code",[t._v("assert False")]),t._v("，这样是避免进行了错误的判断，而最后一句则是为了IDE友好（特指pycharm）")])]),t._v(" "),a("p",[t._v("此时的目录结构就变成了：")]),t._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("div",{staticClass:"highlight-lines"},[a("div",{staticClass:"highlighted"},[t._v(" ")]),a("br"),a("br")]),a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("├─  datasets.py\n└─  main.py\n")])]),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br")])]),a("p",[t._v("如果数据集还会更多，而且每个数据集内部的调用和处理过程也会很繁琐，那么单独建立一个 "),a("code",[t._v("datasets")]),t._v(" 文件夹，将不同的数据集放在不同的.py 文件中，可能是一个好的选择。总之，核心逻辑是"),a("code",[t._v("将变化拆分到文件中")]),t._v("。")]),t._v(" "),a("h3",{attrs:{id:"扩展模型结构"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#扩展模型结构","aria-hidden":"true"}},[t._v("#")]),t._v(" 扩展模型结构")]),t._v(" "),a("p",[t._v("之后继续扩展，你可能会在训练 "),a("code",[t._v("CIFAR10")]),t._v("的时候更换模型结构，此时参数关联就起到了作用，可以让你无需每次设置两个参数：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{attrs:{class:"token comment"}},[t._v("#              参数1   参数1值   参数2  参数2值")]),t._v("\nparams"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bind"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token string"}},[t._v("'dataset'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{attrs:{class:"token string"}},[t._v("'mnist'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{attrs:{class:"token string"}},[t._v("'arch'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{attrs:{class:"token string"}},[t._v("'simple'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nparams"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bind"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token string"}},[t._v("'dataset'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{attrs:{class:"token string"}},[t._v("'fmnist'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{attrs:{class:"token string"}},[t._v("'arch'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{attrs:{class:"token string"}},[t._v("'simple'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nparams"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bind"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token string"}},[t._v("'dataset'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{attrs:{class:"token string"}},[t._v("'cifar10'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{attrs:{class:"token string"}},[t._v("'arch'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{attrs:{class:"token string"}},[t._v("'cnn13'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br")])]),a("p",[t._v("这时依然遵守"),a("strong",[t._v("将变化拆分到文件中")]),t._v("的逻辑，多个模型就需要额外寻找文件进行存放，然后用一些方式将其引入。在这里，由于每个模型的构造相对代码行数较长，因此比较推荐一个模型存放到一个"),a("code",[t._v(".py")]),t._v("文件，所有的模型存放到"),a("code",[t._v("arch")]),t._v("/"),a("code",[t._v("architecture")]),t._v("目录下。")]),t._v(" "),a("p",[t._v("此时文件结构会变成：")]),t._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("div",{staticClass:"highlight-lines"},[a("br"),a("br"),a("br"),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("br"),a("br")]),a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("├─ datasets.py\n├─ main.py\n│\n└─arch\n  ├─ cnn13.py\n  ├─ simple.py\n  └─ __init__.py\n")])]),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br")])]),a("p",[t._v("相应的加载模型的位置可能会变成")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("div",{staticClass:"highlight-lines"},[a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("br"),a("br")]),a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("    "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("models")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" params"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Params"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("from")]),t._v(" torch"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optim "),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SGD\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("from")]),t._v(" arch "),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" cnn13"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("simple\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("if")]),t._v(" params"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("arch "),a("span",{attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v("'cnn13'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" cnn13"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("CNN13"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" params"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("arch "),a("span",{attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v("'simple'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" simple"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("SimpleNet"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("else")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{attrs:{class:"token boolean"}},[t._v("False")]),t._v("\n\n        self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("device"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optim "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" SGD"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parameters"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("**")]),t._v("params"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optim"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br")])]),a("div",{staticClass:"tip custom-block"},[a("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),a("p",[t._v("你可以直接通过 trainer.to(device) 来为所有的模型设置 device，其内部实现通过Python的魔法函数来完成。")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("to")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" device"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" k"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" v "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_model_dict"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("items"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__setattr__"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("k"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" v"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("device"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br")])])]),t._v(" "),a("div",{staticClass:"tip custom-block"},[a("p",{staticClass:"custom-block-title"},[t._v("transform 放在哪？")]),t._v(" "),a("p",[t._v("数据集一般都要进行一些预处理操作，如果是简单的预处理，那么直接写在数据集加载的方法内就可以；如果是复杂的，那么或许在 "),a("code",[t._v("datasets")]),t._v(" 文件夹内建立一个 "),a("code",[t._v("transform")]),t._v(" 用来应对各个数据集的预处理方法是一个更好的选择")])]),t._v(" "),a("h3",{attrs:{id:"扩展-trick"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#扩展-trick","aria-hidden":"true"}},[t._v("#")]),t._v(" 扩展 trick")]),t._v(" "),a("p",[t._v("一个很好用的 trick 是 EMA（指数平滑），由于一个实验可能会添加很多类似的 trick，你可以单独建立一个文件夹命名为 "),a("code",[t._v("tricks")])]),t._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("div",{staticClass:"highlight-lines"},[a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("br"),a("br")]),a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v(" ├─ datasets.py\n ├─ main.py\n │\n ├─arch\n │   ├─ cnn13.py\n │   ├─ simple.py\n │   └─  __init__.py\n │\n └─tricks\n     ├─ ema.py\n     └─ __init__.py\n")])]),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br")])]),a("p",[t._v("EMA 的更新操作是在每一个 batch 训练完后进行，你可以将该更新方法写在 "),a("code",[t._v("train_batch()")]),t._v(" 方法中，但如果要更好的表明 ema_model 的更新过程是在一个一次迭代后，或者说为了更好的分离核心的训练逻辑，可以利用 Python 中的多重继承，让 "),a("code",[t._v("MNISTTrainer")]),t._v(" 同时继承"),a("code",[t._v("TrainCallback")])]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("div",{staticClass:"highlight-lines"},[a("div",{staticClass:"highlighted"},[t._v(" ")]),a("br"),a("br"),a("br"),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("br"),a("br"),a("br"),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("br"),a("br"),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("br"),a("br"),a("br")]),a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{attrs:{class:"token class-name"}},[t._v("MNISTTrainer")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Trainer"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" callbacks"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("TrainCallback"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("models")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" params"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Params"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("if")]),t._v(" params"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ema"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tricks"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ema "),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" ema\n            self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ema_model "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" ema"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n    "),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("on_train_batch_end")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" trainer"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v("'BaseTrainer'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" func"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" params"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Params"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" meter"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Meter"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("*")]),t._v("args"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("**")]),t._v("kwargs"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("if")]),t._v(" params"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ema"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tricks"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ema "),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" update_ema_variables\n            update_ema_variables"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ema_model"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" params"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("global_step"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("predict")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" xs"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("if")]),t._v(" params"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ema"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ema_model"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xs"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{attrs:{class:"token keyword"}},[t._v("else")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xs"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br")])]),a("h2",{attrs:{id:"什么时候需要多个trainer？"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#什么时候需要多个trainer？","aria-hidden":"true"}},[t._v("#")]),t._v(" 什么时候需要多个Trainer？")]),t._v(" "),a("p",[t._v("通过之前的过程我们可以看到，当数据集或模型结构有变化的时候，我们只需要将其变化拆分到其他文件中，随后在参数中添加选项，就可以保持良好的代码结构，这个时候即使只有一个 "),a("code",[t._v("Trainer")]),t._v(" 也可以，如果让每个 "),a("code",[t._v("Trainer")]),t._v(" 对应不同的数据集或模型结构，反而会导致代码的冗余。")]),t._v(" "),a("p",[t._v("那么什么时候需要多个 "),a("code",[t._v("Trainer")]),t._v(" 呢？深度学习三件套，数据、模型、算法，当算法改变，也即"),a("code",[t._v("train_batch()")]),t._v("方法中的逻辑都要修改时，你可能就需要考虑使用多个 "),a("code",[t._v("Trainer")]),t._v(" 了。")]),t._v(" "),a("p",[t._v("比如说，在交叉熵损失函数的基础上，你想要添加三元组损失函数，此时数据的加载方式肯定会大幅改变，由 "),a("code",[t._v("(xs,ys)")]),t._v(" 转变成 "),a("code",[t._v("(xs,pos,neg,ys)")]),t._v("。对于数据集的变化，你可以通过在相应的文件夹中添加 "),a("code",[t._v("triplet")]),t._v(" 文件或文件夹（这取决于你要改动的数据集数量）。而在 "),a("code",[t._v("Trainer")]),t._v(" 中，你会有两个选择：")]),t._v(" "),a("ul",[a("li",[t._v("通过在 params 中添加可选项，在 "),a("code",[t._v("train_batch()")]),t._v(" 逻辑中进行区分")]),t._v(" "),a("li",[t._v("继承创建另外一个 "),a("code",[t._v("Trainer")]),t._v(" （这个时候，可能需要单独创建一个"),a("code",[t._v("trainer")]),t._v("文件存放每一种"),a("code",[t._v("Trainer")]),t._v("），用于训练添加了三元组损失函数的模型")])]),t._v(" "),a("p",[t._v("这两种方法没有优劣之分，决定采用哪一种选择的关键在于，你认为你新添加的这一方法，是另一个实验，还是该实验的一个对比实验？"),a("code",[t._v("thexp")]),t._v(" 在对目录组织的分级中明确了项目和实验的关系（一个项目可以对应多个实验），而在"),a("code",[t._v("Trainer")]),t._v("中，也存在一个类变量 "),a("code",[t._v("__exp_name__")]),t._v(" 用来标识该 "),a("code",[t._v("Trainer")]),t._v(" 的实验名。因此，是否拆分 "),a("code",[t._v("Trainer")]),t._v(" ，由你这项更改的性质决定。")]),t._v(" "),a("div",{staticClass:"tip custom-block"},[a("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),a("p",[t._v("比如，一个sota的代码中会存在对之前其他论文代码的复现实验，这明显属于不同的实验，因此多个 "),a("code",[t._v("Trainer")]),t._v(" 肯定是更好的选择。")])]),t._v(" "),a("p",[t._v("当 "),a("code",[t._v("Trainer")]),t._v(" 被拆分的时候，出于文件组织的考虑，运行主函数就不推荐写在 "),a("code",[t._v("Trainer")]),t._v(" 定义的文件中。此时，将主运行文件放在项目根目录下，一个 "),a("code",[t._v("Trainer")]),t._v("（也是一个实验）对应一个运行文件，可能会更好一些。在这种情况下，数据、模型、算法、运行被很好的区分开，保证了代码结构尽可能的扁平化，一处的更改不会引起其他文件的大幅度更改。")]),t._v(" "),a("h2",{attrs:{id:"在全局使用-params"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#在全局使用-params","aria-hidden":"true"}},[t._v("#")]),t._v(" 在全局使用 params")]),t._v(" "),a("p",[t._v("实际上，如果你控制好引用关系避免递归引用，你完全可以将 params 变成全局的参数。虽然"),a("code",[t._v("exp")]),t._v(" 中也提供了运行期间的全局参数（再次觉得这可能是个鸡肋），但 "),a("code",[t._v("params")]),t._v(" 在项目中定义的变量将是 "),a("code",[t._v("IDE")]),t._v(" 友好型的，能够在全局被自动补全检测到。")]),t._v(" "),a("div",{staticClass:"tip custom-block"},[a("p",{staticClass:"custom-block-title"},[t._v("Params放在哪？")]),t._v(" "),a("p",[t._v("当你将 params 作为全局参数的时候，比较推荐单独将其放在一个文件中，命名为 "),a("code",[t._v("config.py")]),t._v(" 或 "),a("code",[t._v("params.py")]),t._v(" 之类的。随后在各个文件中引入并使用。")])]),t._v(" "),a("h3",{attrs:{id:"是否有必要定义多个-params"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#是否有必要定义多个-params","aria-hidden":"true"}},[t._v("#")]),t._v(" 是否有必要定义多个 Params")]),t._v(" "),a("p",[t._v("当定义了多个 "),a("code",[t._v("Trainer")]),t._v(" 的时候，很有必要考虑一下是否有必要为每个 "),a("code",[t._v("Trainer")]),t._v(" 定义不同 "),a("code",[t._v("Params")]),t._v("。不断的往一个 "),a("code",[t._v("Params")]),t._v(" 中添加参数固然统一，但却有可能导致每个实验会保存大量的冗余参数（只是有可能）。")]),t._v(" "),a("p",[t._v("一个可选项是利用继承属性，在基类中定义共有的变量，在子类中定义特有的变量。但这种方式，没有办法很好的将"),a("code",[t._v("params")]),t._v("作为全局变量。")]),t._v(" "),a("p",[t._v("一个还算推荐的写法是仍然将全局参数写在 "),a("code",[t._v("config.py")]),t._v(" 中，随后在各个 "),a("code",[t._v("Trainer")]),t._v(" 文件中引入并添加特有的变量。这样可以比较好的保证各个 "),a("code",[t._v("Trainer")]),t._v(" 之间的参数不会互相污染。")]),t._v(" "),a("div",{staticClass:"tip custom-block"},[a("p",{staticClass:"custom-block-title"},[t._v("使用权和修改权")]),t._v(" "),a("p",[a("code",[t._v("Trainer")]),t._v(" 最好和文件是一一对应的关系，没有必要在一个文件中包含多个 "),a("code",[t._v("Trainer")]),t._v("。基于此，"),a("code",[t._v("Params")]),t._v(" 应该只在定义文件 "),a("code",[t._v("config.py")]),t._v(" 和 各个 "),a("code",[t._v("Trainer")]),t._v(" 文件中进行修改，在其他文件中，应该只对其进行读取，不应该对其进行修改。")])]),t._v(" "),a("h2",{attrs:{id:"扩展thexp功能"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#扩展thexp功能","aria-hidden":"true"}},[t._v("#")]),t._v(" 扩展thexp功能")]),t._v(" "),a("p",[t._v("你可能想对"),a("code",[t._v("thexp")]),t._v(" 提供的回调功能进行扩展，或想添加 "),a("code",[t._v("thexp")]),t._v(" 中提供的某些分类的功能。此外，由于 "),a("code",[t._v("thexp")]),t._v(" 缺少测试，那么或许对于其中的一些功能，你使用的时候会出现一些小bug，你会在本地新建一个同名文件来修改好这个bug。")]),t._v(" "),a("p",[t._v("在这种情况下，我是统一将其放在 "),a("code",[t._v("thextra")]),t._v(" 文件夹下。其作用是提醒我：在结束这个实验后，记得把这个文件夹里的东西添加到 "),a("code",[t._v("thexp")]),t._v(" 上去。")])])}],!1,null,null,null);r.options.__file="2-structure.md";s.default=r.exports}}]);